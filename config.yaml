# ===============================================================
# HUDI CATALOG SYNC TEST CONFIGURATION
# ---------------------------------------------------------------
# Central configuration used for syncing Hudi tables with:
#   • Hive Metastore
#   • Google BigQuery
#   • AWS Glue Catalog
#   • DataHub
#
# All sync tools inherit common values from `global`
# ===============================================================

# ===============================================================
# GLOBAL CONFIGURATION
# ===============================================================
global:
  # ---------------- Environment Variables ----------------
  spark_home: ''
  spark_master: "local[4]"
  spark_version: "3.4.4"
  scala_version: "2.12"
  hudi_version: "0.16.0-SNAPSHOT"

  # ---------------- Table Settings ----------------
  table_type: "COPY_ON_WRITE"                   # Table type: "COPY_ON_WRITE" (e.g. "COPY_ON_WRITE")
  base_file_format: "PARQUET"                   # Base file format: "PARQUET" (e.g. "PARQUET")
  base_table_name: "stock_ticks"
  database_name: "default"                      # Database name: "default" (e.g. "default")

  # ---------------- Storage Paths Settings --------------------
  base_table_path: "/tmp/hudi_catalog_sync/tables"    # Base table path: "tables" (e.g. "tables")
  base_data_path: "/tmp/hudi_catalog_sync/data"       # For streamer: props, schema.avsc, and source root (e.g. gs://bucket/data or s3a://bucket/data)
  
  # ---------------- JARs Path Settings --------------------
  jars_path: "/tmp/hudi_catalog_sync/jars"            # Base path for Hudi JARs (e.g. gs://bucket/jars or s3a://bucket/jars)
  
  # ---------------- Partition Settings ---------------- 
  partition_fields: "date"                            # Partition fields (e.g. "date")  
  partition_value_extractor: "org.apache.hudi.hive.SlashEncodedDayPartitionValueExtractor" # Partition value extractor (e.g. "org.apache.hudi.hive.SlashEncodedDayPartitionValueExtractor")
  record_key_field: "symbol"                          # Record key field (e.g. "symbol")
  precombine_field: "ts"                              # Precombine field (e.g. "ts")
  partition_path_field: "date"                        # Partition path field (e.g. "date")
  keygenerator_type: "SIMPLE"                         # Key generator type (e.g. "SIMPLE")
  hive_style_partitioning: true                       # Hive style partitioning (e.g. true)
  metadata_enable: true                               # Metadata enable (e.g. true)
 
  # ---------------- Extra Spark Packages Settings --------------------
  extraclasspath_enabled: false                       # Extra classpath enabled (e.g. true)
  packages: ""                                        # Extra packages

  # Hive Sync Settings
  mode: "hms"                                         # Sync mode: "hms" (thrift) or "jdbc"
  metastore_uris: "thrift://localhost:9083"           # Metastore URIs for mode=hms

# ===============================================================
# HUDI CATALOG SYNC TEST CONFIGURATION
# ===============================================================
hive:
  enabled: true
  sync_tool_class: "org.apache.hudi.hive.HiveSyncTool"
  database: "default"                                 # Hive database name (e.g. "default") # Hive base path (e.g. "/tmp/hudi_catalog_sync/tables/stock_ticks_hive_separate_0_16_0_2026_02_27")

# ===============================================================
# BIGQUERY CATALOG SYNC TEST CONFIGURATION
# ===============================================================

bigquery:
  enabled: true
  sync_tool_class: "org.apache.hudi.gcp.bigquery.BigQuerySyncTool"
  project_id: "infra-dev-358110"                      # BigQuery project ID (e.g. "infra-dev-358110")
  dataset_name: "hudi_release_testing"                # BigQuery dataset name (e.g. "hudi_release_testing")
  dataset_location: "us-central1"                     # BigQuery dataset location
  source_uri: ""                                      # Source URI (e.g. "gs://bucket/path/table/date=*")
  source_uri_prefix: ""                               # Source URI prefix (e.g. "gs://bucket/path/table/")
  use_file_listing_from_metadata: true                # Use file listing from metadata (e.g. true)
  assume_date_partitioning: false                     # Assume date partitioning  
  use_bq_manifest_file: true                          # Use BQ manifest file (e.g. true)
  extraclasspath_enabled: true
  packages: >
    com.google.cloud:google-cloud-bigquery:2.44.0,
    com.google.api-client:google-api-client:1.32.1,
    com.google.http-client:google-http-client-jackson2:1.39.2

# ===============================================================
# AWS GLUE CATALOG SYNC TEST CONFIGURATION
# ===============================================================
glue:
  enabled: true
  sync_tool_class: "org.apache.hudi.aws.sync.AwsGlueCatalogSyncTool"
  database: "hudi_db"                                 # Glue database name (e.g. "hudi_db")
  hive_sync_database: "hudi_db"                       # Hive database name (e.g. "hudi_db")
  hive_sync_partition_fields: "date"                  # Hive partition fields (e.g. "date") 
  hive_sync_partition_extractor_class: "org.apache.hudi.hive.SlashEncodedDayPartitionValueExtractor" # Hive partition value extractor (e.g. "org.apache.hudi.hive.SlashEncodedDayPartitionValueExtractor")
  meta_sync_condition_sync: true                      # Meta sync condition sync (e.g. true)

# ===============================================================
# DATAHUB CATALOG SYNC TEST CONFIGURATION
# ===============================================================
datahub:
  enabled: true
  sync_tool_class: "org.apache.hudi.sync.datahub.DataHubSyncTool"
  emitter_server: "http://localhost:8080"             # DataHub emitter server (e.g. "http://localhost:8080")  
  database: "datahub_db"                              # DataHub database name (e.g. "datahub_db")
  emit_log_metrics: true                              # Emit log metrics (e.g. true)
  schema_string_length_thresh: 4000                   # Schema string length threshold (e.g. 4000)
